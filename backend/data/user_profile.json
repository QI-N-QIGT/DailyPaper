{
    "suggested_queries": [
        "Mixture-of-Experts 3D Near-Memory Processing",
        "Transformer-in-Memory accelerator eDRAM",
        "Dynamic expert parallelism LLM inference",
        "Sparsity-aware quantization for PIM architectures",
        "Hybrid Tensor-Expert parallelism optimization",
        "3D stacked DRAM AI inference efficiency",
        "Node-Link balance co-optimization MoE",
        "Bit-slice sparsity hardware acceleration"
    ],
    "research_directions": [
        "Hardware-Algorithm Co-design for MoE Inference: Optimizing Mixture-of-Expert models through hybrid parallelism and dynamic scheduling specifically for distributed memory architectures like 3D Near-Memory Processing (NMP).",
        "Processing-in-Memory (PIM) for Transformers: Developing specialized hardware accelerators using high-density 2T1C eDRAM and heterogeneous cores to minimize data movement and maximize energy efficiency for large-scale Transformer models.",
        "Dynamic Expert Placement and Scheduling: Leveraging temporal locality and activation patterns in LLMs to proactively manage workload balance and reduce communication overhead in distributed systems.",
        "Sparsity-Aware Quantization and Compute: Exploiting both structured and unstructured bit-slice sparsity in weights and activations to enhance throughput and reduce memory footprint via specialized quantization schemes and dense-sparse hardware engines."
    ],
    "updated_at": 1767973403.8069923
}