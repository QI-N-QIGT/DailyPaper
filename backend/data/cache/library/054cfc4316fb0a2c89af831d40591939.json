{
  "research_directions": [
    "Processing-In-Memory (PIM) and Near-Memory Processing (NMP): Architectures utilizing DRAM, eDRAM, and HBM2 to mitigate the memory wall bottleneck in AI accelerators.",
    "Efficient Mixture-of-Expert (MoE) Inference: Optimizing the deployment of large-scale MoE models on distributed memory systems through hybrid and dynamic parallelism.",
    "Sparsity-Aware Transformer Acceleration: Hardware-software co-design using sparsity-aware quantization and heterogeneous cores to handle sparse matrix multiplications in Transformers.",
    "System-Level Interconnect and Communication Optimization: Reducing data movement and NoC congestion in 3D-stacked and 2D-mesh PIM architectures through advanced routing and load balancing.",
    "Hardware for LLM Efficiency: Specialized silicon and circuit-level innovations (e.g., 2T1C cells, programmable PCUs) designed to improve TOPS/W for large language model workloads."
  ],
  "suggested_queries": [
    "3D Near-Memory Processing Mixture-of-Experts",
    "Processing-in-memory Transformer acceleration eDRAM",
    "Hybrid Parallelism MoE LLM inference NMP",
    "Sparsity-aware quantization for PIM accelerators",
    "HBM2 Function-In-Memory bank-level parallelism",
    "NoC communication optimization for 3D-stacked DRAM",
    "2T1C eDRAM PIM for Transformer inference"
  ]
}